# -*- mode: ruby -*-
# vi: set ft=ruby :
#

## Create Local Multi-node Kubernetes Clusters In One Simple Command
# 
# This project has a simple aim: make is easy to create multi-node Kubernetes clusters, in a local environment,
# with a single command.  Also, to supports several clusters, and easy switching between them.

# Why use this project and not another?  Because the steps are clear and uncomplicated -- maybe you will find it
# easy for you, as well.  My interest, beyond taking it as a modest "Infrastructure as Code" personal challenge,
# was to:
# * Avoid a sometimes complex, often confusing, process of deployment via other solutions.
# * Applying a "canonical" Kubernetes deployment solution, kubeadm, whose steps can be easily seen to be consistent
#   with the steps appearing in kubernetes.io for kubeadm (actually copied/pasted from there, whenever possible).
# * Combine and consolidate the steps scattered across kubernetes.io's kubeadm setup webpage (kubeadm, CRI, CNI, etc),
#   to a single command.
# * Implement it such that discarding, and recreating, a working cluster is trivial, reliable, and fast.
# * Document the tools, applications needed (where to find, what, and how, to install).
# * Report the issues discovered in the development process, and catalogue their resolution.
# * Support both Linux and Windows 10 WSL (Windows Subsystem for Linux) hosts.
# * Package it as a small bundle of files.
# 
# What's the downside?
# * Your environment may be different than mine, causing you to encounter problems that I've promised you would be
#   avoiding.  Sorry if that occurs.
# * You may not agree with the tools I specify, or the changes suggested in your environment
# * You may feel limited by my choice of operating systems, cluster configuration, etc. (Let me know).
# * This project may fall behind kubeadm advances/changes in kubernetes.io.  C'est la vie.
# 
# All the files that comprise this project are in https://github.com/dsloyer/ez-kubeadm
# 
## Overview
# * Kubeadm is the tool used to deploy the cluster.
# * Vagrant installs and configures the Ubuntu/CentOS boxes on VirtualBox.
# * Bash scripts manage the process, perform further operations on the cluster nodes, providing a seamless experience.
# * To better support multiple kubernetes configurations, we modify the kubeconfig files, gather them in a single
#   directory, and set the KUBECONFIG env var based on the contents of that directory.
# * I like to ssh directly into the cluster from any directory on my host, which is enabled by pushing an SSH public
#   key down to each node.
# 
# The Kubernetes master and worker node VMs can be either Ubuntu (by default; CentOS is easily selected via runtime
# parameter).
# 
# Currently supporting any of five networking alternatives, a CNI network is selected via environment variable -- 
# one of {calico, canal, flannel, romana, weave}.  Calico is deployed by default.  You may not care what network is
# being used -- I included them as an exercise for me.  If you're interested in exploring, for example, network policies,
# you may find one network, or another, to be of interest.
# 
# A complete Kubernetes cluster based on Ubuntu, with Calico networking, can be built with a single command in the
# vagrant project directory. Before running that command, you first must do some modest preparation:
#   1. clone this repository locally (into, say $HOME/projects/ez-kubeadm)
#   2. install vagrant and VirtualBox
#   3. setup directories and env variables. All this is described in detail, below.
# 
# As of mid-March, 2019, this script creates a 3-node k8s cluster (master and 2 worker nodes), with these versions:
#   * Kubernetes: 1.13.4                          (current version on kubernetes.io)
#   * Docker:     18.06.2                         (prescribed by kubernetes.io)
#   * Centos:     CentOS7,                        (prescribed by kubernetes.io))
#     * Version:  1902.01                         (latest CentOS7 box from Vagrant)
#   or
#   * Ubuntu:     Ubuntu/xenial64                 (prescribed by kubernetes.io)
#     * Version   20190308.0.0                    (latest Ubuntu Xenial box from Vagrant)
#     
# ## Show Me
# Assuming you've setup your system per the instructions below, a few simple steps prepare for an entirely new cluster.
# Let's call it 'ukube':
# ```
# $ cd $HOME/projects
# $ mkdir ukube && cd ukube
# $ vagrant init
# $ cp ../ez-kubeadm/* .
# $ cp $HOME/.ssh/id_rsa.pub id_rsa.pub.$LOGNAME
# ```
# You're ready to create a cluster, or destroy and recreate an Ubuntu-based Kubernetes cluster, anytime, from this directory,
# # with a single command:
# ```
# $ source ./makeK8s.sh
# ```
# About 10 minutes later, the cluster is up and ready to use:
# ```
# $ kubectl config use-context ukube
# Switched to context "ukube".
# $ kubectl get nodes
# NAME     STATUS   ROLES    AGE     VERSION
# master   Ready    master   5m41s   v1.13.4
# node1    Ready    <none>   3m8s    v1.13.4
# node2    Ready    <none>   21s     v1.13.4                                                                             
# ```
# Let's change directory, spin up a BusyBox container on each node as a daemonset (ds-bb.yaml not shown here), ssh
# to the master node, and list the running pods:
# ```
# $ cd $HOME/test
# $ kubectl create -f ds-bb.yaml
# daemonset.apps/bb created
# 
# $ ssh master
# The authenticity of host 'master (192.168.205.10)' can't be established.
# ECDSA key fingerprint is SHA256:lHvDa0Gg2wnjGeD7rmXWw5ltSGzc8OCnewi9xJpDfXE.
# Are you sure you want to continue connecting (yes/no)? yes
# Warning: Permanently added 'master' (ECDSA) to the list of known hosts.
# Welcome to Ubuntu 16.04.6 LTS (GNU/Linux 4.4.0-142-generic x86_64)
# --- some text removed ---
# 
# username@master:~$ kubectl get po -o wide
# NAME       READY   STATUS    RESTARTS   AGE   IP            NODE     NOMINATED NODE   READINESS GATES
# bb-2jc26   1/1     Running   0          32s   192.168.0.5   master   <none>           <none>
# bb-7xhkx   1/1     Running   0          32s   192.168.1.3   node1    <none>           <none>
# bb-d684p   1/1     Running   0          32s   192.168.2.4   node2    <none>           <none>
# ```
# Destroy the cluster by cd'ing into the project folder for the cluster, then run:
# ```
# $ cd $HOME/projects/ukube
# $ vagrant destroy -f
# ```
# 
# To create a cluster built from CentOS and Flannel networking:
# ```
# $ export k8snet=flannel
# $ source ./makeK8s.sh -s centos
# ```
# 
# ## Setup Instructions:
# The setup for native Linux and Windows WSL is virtually identical, other than a few additional commands I've detailed just below.
# 
#   1. Install kubectl on your host system, per instructions on kubernetes.io
#      One method (https://kubernetes.io/docs/tasks/tools/install-kubectl/):
#        ```
#        $ curl -LO https://storage.googleapis.com/kubernetes-release/release/$(curl -s https://storage.googleapis.com/kubernetes-release/release/stable.txt)/bin/linux/amd64/kubectl
#        $ chmod +x kubectl
#        $ sudo mv kubectl /usr/local/bin
#        ```
#      Make it executable and move to a preferred directory in your path, e.g. /usr/local/bin, as seen above
#   2. Install VirtualBox 5.2.6 for your system (VirtualBox 6.0 has only given unresolvable problem).
#      On Linux, install VirtualBox for Linux. For Windows WSL, install the Windows version, not the Linux version.
#      NOTE: We assume you've enabled virtualization in the BIOS, and that no conflicting virtualization schemes have
#      been enabled (e.g. Windows Hyper-V)
#   3. [WSL only] Add VirtualBox binaries to System PATH, found at
#        System->Properties->Adv System Settings->Environment Variables...->System variables
#      The VirtualBox path is typically c:\Program Files\Oracle\VirtualBox, which you append to the System PATH.
#      
#      As discussed below, it's a good idea to locate the projects directory in, e.g., C:\Users\$LOGNAME\projects,
#      set a symlink from /home/$LOGNAME to that projects directory, set env vars also listed here, and specify metadata
#      on the mounted C: drive:
#      ```
#      $ ln -s /mnt/c/Users/$LOGNAME/projects $HOME/projects
#      $ echo "export VAGRANT_WSL_WINDOWS_ACCESS_USER_HOME_PATH=/home/$LOGNAME/projects" >>$HOME/.bashrc && source $HOME/.bashrc
#      $ echo "export VAGRANT_WSL_ENABLE_WINDOWS_ACCESS=1" >>$HOME/.bashrc && source $HOME/.bashrc
#      $ echo "export VAGRANT_HOME=\"$HOME/.vagrant.d\""      >>$HOME/.bashrc && source $HOME/.bashrc
#      $ sudo umount /mnt/c && sudo mount -t drvfs C: /mnt/c -o metadata
#      ```
#   4. Install vagrant for Linux in bash (both Linux and Windows WSL). I used
#      ```
#      $ wget https://releases.hashicorp.com/vagrant/2.2.4/vagrant_2.2.4_x86_64.deb
#      $ sudo apt-get install ./vagrant_2.2.4_x86_64.deb
#      ```
#   5. Clone ez-kubeadm into your projects directory, e.g. #HOME/projects/ez-kubeadm.
#   6. Create a project directory specific to the new kubernetes cluster; e.g. $HOME/projects/ukube/.
#   7. Accept ez-kubeadm's default directory for kubeconfig files -- $HOME/.kube/config.d. This can be over-ridden
#      by using the "-o" option to makeK8s.sh. If you accept the default directory, create it:
#      ```
#      $ mkdir -p $HOME/.kube/config.d
#      ```
#   8. cd to the specific kubernetes cluster project directory, e.g. $HOME/projects/ukube
#   9. Run "vagrant init"
#      ```
#      $ vagrant init
#      ```
#   10. Copy the collection of files from this repo into the project directory
#       ```
#       $ cp ../ez-kubeadm/* .
#       ```
#   11. Copy your id_rsa.pub file into the vagrant project folder (if needed, use "ssh-keygen -t rsa -b 4096 -f id_rsa" in
#       $HOME/.ssh)
#       ```
#       $ cp $HOME/.ssh/id_rsa.pub id_rsa.pub.$LOGNAME
#       ```
#   12. Consider the CPU/memory settings in the relevant Vagrantfile -- either Vagrantfile.ubuntu, or Vagrantfile.centos.
#       Preferring Ubuntu, I've set RAM on Ubuntu nodes to 4096MB, while CentOS nodes get only 2048MB, unless changed
#       in the Vagrantfile.
#   13. Run "source ./makeK8s.sh", or "source ./makeK8s.sh -s centos" to create a new cluster
#   
# ## Notes 
#   1. Edits to the Vagrantfile (Vagrantfile.ubuntu or Vagrantfile.centos) should only be needed to:
#      * change the memory or CPU settings for the nodes
#      * change master and worker node IP addresses.
#        Ubuntu master IP is 192.168.205.10; worker node IPs immediately follow, i.e. node1 is 192.168.205.11
#        CentOS cmaster IP is 192.168.205.15; worker node IPs immediately follow, i.e. cnode1 is 192.168.205.16
#      * want more/fewer nodes? edit the servers array in the Vagrantfile.
# 
#   2. To set the KUBECONFIG env var at any time, e.g. on a new shell instance, cd to the project directory, and
#      "source" the script, setKubeConfigVar.sh:
#      ```
#      $ source ./setKubeConfigVar.sh
#      ```
#      Consider copying this script into your path, somewhere.
#   3. Only one context can be active at a time, across multiple shells.
#   4. Several clusters can exist at any point in time.  View available configs using:
#      ```
#      $ kubectl config get-contexts
#      CURRENT   NAME       CLUSTER     AUTHINFO      NAMESPACE
#                ckube      ckube-clu   ckube-admin
#                minikube   minikube    minikube
#      *         ukube      ukube-clu   ukube-admin
#      ```
#   5. Select context (project name) using
#       ```
#       $ kubectl config use-context <context-name>
#       ```
#   6. I suggest adding the node names (master, node1, etc) to your hosts file. Actually, the some of the
#      bash script logic depends on it.
#      
#      In Windows, these changes are applied to the native Windows hosts file -- not /etc/hosts in bash.
#      The native Windows hosts file can be found at C:\Windows\system32\drivers\etc\hosts.
#   7. When you are entirely finished with a cluster, the kubeconfig file will remain after its destruction;
#      as such, you will want to delete it from the directory where these files are kept -- $HOME/.kube/config.d.
#   
# ## WSL Notes (running these scripts on Windows 10's Linux environment):
# 
# My development and testing were initially performed on Ubuntu 18 (Bionic). I later ported it to 
# Windows 10's WSL Ubuntu (bash) environment.
# 
# There were serveral changes required to get things working on WSL -- some in the Vagrantfiles, some in the
# Windows environment.  Thankfully, the required file changes for WSL are compatible with native Ubuntu, so
# we don't need any Windows-specific files.
# 
# I've tried to capture all necessary steps here. I suggest reviewing: https://www.vagrantup.com/docs/other/wsl.html.
# 
# Specific env vars and for Windows WSL:
#   1. Set your WSL project environment -- vagrant projects don't work well from /home/<user>. I suggest basing
#      your projects in, for example, /mnt/c/Users/<user>/projects, but use a symlink in a directory under your
#      home directly, as suggested here:
#        https://cepa.io/2018/02/20/linuxizing-your-windows-pc-part2/
#      Let's make that more concrete:
#        My username is $LOGNAME; $HOME is /home/$LOGNAME; my projects directory in Windows is
#        C:\Users\$LOGNAME\projects, where my vagrant project folders live. Assume that we want to access 
#        that directory from $HOME/projects. Use a symlink to accomplish this.
#        Make the vagrant project folders accessible from my $HOME directory:
#        ```
#        ln -s /mnt/c/Users/$LOGNAME/projects $HOME/projects
#        ```
#   * export VAGRANT_WSL_WINDOWS_ACCESS_USER_HOME_PATH, and append to .bashrc
#        Set the root path to your vagrant projects directory by exporting this env var, and append to .bashrc:
#        ```
#        echo "export VAGRANT_WSL_WINDOWS_ACCESS_USER_HOME_PATH=/home/$LOGNAME/projects" >>$HOME/.bashrc && source $HOME/.bashrc
#        ```
#   * export VAGRANT_WSL_ENABLE_WINDOWS_ACCESS, and append to .bashrc
#      ```
#      echo "export VAGRANT_WSL_ENABLE_WINDOWS_ACCESS=1" >>$HOME/.bashrc
#      ```
#   * To avoid rsync and vagrant ssh problems (e.g. "error when attempting to rsync a synced folder":
#      ```
#      echo "export VAGRANT_HOME=\"/home/$LOGNAME/.vagrant.d\"" >>$HOME/.bashrc && source $HOME/.bashrc
#      ```
#      Note: In Vagrantfile, I've added this line to avoid another rsync issue: config.ssh.insert_key = false
#   2. C:\Windows\System32\drivers\etc\hosts file permissions -- user must have modify permission
#      to avoid "Permission denied" for the vagrant hostsupdater plugin to work (it's not installed by
#      VBox 5.2.x, but is in VBox 6.0.)
#   3. Mounted Windows partitions, e.g. C:, may ignore permissions set, for example, by chmod. Correct this
#       by re-mounting the volume, specifying "-o metadata", viz.:
#       ```
#        $ sudo umount /mnt/c && sudo mount -t drvfs C: /mnt/c -o metadata
#       ```
#       Also, the files in /mnt/c may all be owned by root. Adjust as needed.
#       For more, see https://blogs.msdn.microsoft.com/commandline/2018/01/12/chmod-chown-wsl-improvements/
#       Also: https://docs.microsoft.com/en-us/windows/wsl/wsl-config
#   4. With permissions changes enabled from bash (via metadata), tighten any ssh key permissions to avoid
#       problems: I set my keys as "chmod 644 id_rsa*"
#   5. vagrant is prone to throwing "Insecure world writable dir" errors. I tend to use "chmod 755 <dir>" to correct
#      these, which seems to work fine.
# 
# That's it for the setup.  Here are some further WSL notes, which describe some issues I've encountered;
# they have all been addressed by the files in our repo:
#   1. There is a problem with some Ubuntu box versions on VM spinup:
#         "rawfile#0 failed to create the raw output file VERR_PATH_NOT_FOUND".
#      The error can be avoided by adding this line to the Vagrantfile:
#        ```
#        vb.customize [ 'modifyvm', :id, '--uartmode1', 'disconnected']
#        ```
#   2. I've observed no need to run as administrator -- neither bash, or VBox Manager
#   3. When this group of files is pulled down from github, they may arrive as DOS-formatted files, which
#      causes runtime errors.  Install and use dos2unix utility to modify the shell scripts, to correct.
#   4. I've had problems with VMs (Xenial) booting extremely slowly with VBox 6.0, while v5.2.26 works great.
#       see https://github.com/hashicorp/vagrant/issues/10578, for a discussion of this issue.
#   5. Another VBox 6.0 issue: Centos cluster VMs don't come up under VBox 6.0 either -- the master node
#       boots fine, but the next VM (cnode1) fails to spin up.
#   6. Side note on VBox 6.0: Windows UAC will trigger when the hostupdater (a vagrant plugin) tries to update
#       the hosts file.
#   7. In short, avoid VBox 6.0.
#  
# ## Repo Files and Network Notes:
# 
# These are the files included in the repo:
#   * makeK8s.sh -- wrapper
#   * Vagrantfiles -- Vagrantfile.centos and Vagrantfile.ubuntu -- one of which is copied to Vagrantfile at runtime.
#   * post-k8s.sh -- make account for host user on nodes, prepare to pull kube config file, admin.conf
#   * pull-k8s-admin.sh -- download admin.conf from master, for use on host
#   * modKubeConfigFile.sh -- process admin.conf file, extracting PKI data, massage naming, and save in directory
#     with other kubeconfig files
#   * setKubeConfigVar.sh -- consolidate multi-cluster configs into KUBECONFIG env var
#   
#   The modified network CNI YAML files are included in this repository. They are:
#   * canal2.yaml, canal2c.yaml -- canal2 for Ubuntu, canal2c for CentOS
#   * kube-flannel.yaml, kube-flannelc.yaml
#   * romana-kubeadm.yaml
# 
# These network CNI's all seem to work well -- feel free to use any of them.  Any quirks have been addressed in the
# Vagrantfiles and YAML:
#   * calico:    Simply works.
#   * weave:     Worker nodes require a static route to the master node (applied by Vagrantfile)
#   * romana:    Seems to require romana-agent daemonset tolerance for not-ready nodes
#   * flannel:   Its yaml must be tweaked to use enp0s8(Ubuntu) or eth1(CentOS) host-only interface, not the NAT'd one
#   * canal      Its yaml must be tweaked to use enp0s8(Ubuntu) or eth1(CentOS) host-only interface, not the NAT'd one
# 
# Calico and Weave need no YAML mods.
# Weave, however, requires a route to be set for worker nodes to find the master (corrected in Vagrantfile).
# Canal, Flannel, and Romana require minor mods their YAML; e.g. use 2nd network adapter (enp0s8/eth1).
# 
# Romana curiously seems to present a catch-22: romana-agent won't install on "not-ready" nodes,
# but the nodes can only become ready when romana-agent is up and running. My solution: add tolerance for
# nodes that are not-ready (applied to the romana-agent daemonset YAML).
# 
# ## SSH key handling
# 
# In building the worker nodes, We run a script on each of the nodes to scp the kubernetes join script
# from the master node (where kubeadm places it, while building the master node).
# This allows automation of the nodes joining the kubernetes cluster.
# 
# To use scp, we use the aforementioned key pair for the vagrant user on each of the nodes, in 
# /home/vagrant/.ssh
# On the master, we also need to add the vagrant pub-key into the master's authorized_keys file, in
# /home/vagrant/.ssh/authorized_keys
# 
# Thankfully, the project directory is automatically mounted onto each node by Vagrant, at /vagrant.
# Therefore, the SSH keys of interest are accessible by all our Vagrant VMs, at that location.
# 
# NOTE: The mount is only automatic during node creation, and must be re-mounted manually if the node reboots.

# OK, let's get to it:

# pull desired network from ENV, if defined; if not, default to calico
$net=ENV['k8snet']
if $net.to_s.strip.empty?
  # default to calico
  $net = "calico"
end

# Set CIDR, as prescribed by kubernetes.io/docs/setup/independent/create-cluster-kubeadm
# These CIDR values agree with the values assumed in the respective YAML files, avoiding
# the need to edit the YAML. If these are changed, YAML must be updated, as well.
if    $net == "flannel"
  $netCidr = "--pod-network-cidr=10.244.0.0/16"
elsif $net == "canal"
  $netCidr = "--pod-network-cidr=10.244.0.0/16"
elsif $net == "calico"
  $netCidr = "--pod-network-cidr=192.168.0.0/16"
elsif $net == "weave"
  $netCidr = ""
elsif $net == "romana"
  $netCidr = ""
end

# Install Ubuntu on all nodes
# Box-specific settings
$grpSuffix   = " - Ubuntu"
$box         = "ubuntu/xenial64"
$boxVer      = "20190308.0.0"
$hostAdapter = "enp0s8"

# Set the desired IP address of the master node, from which node IPs will follow
$masterIp    = "192.168.205.10"

# DEB-based systems use /etc/default/kubelet
# see https://kubernetes.io/docs/setup/independent/kubelet-integration
$cfgKubelet  = "/etc/default/kubelet"

# Pointers to network YAML files:
# The calico network plug-in for kubernetes is installed by applying two yaml files
# The required files can be found at
# https://kubernetes.io/docs/setup/independent/create-cluster-kubeadm/
# On that page, scroll down, elect the calico tab, and observe the required yaml file paths
# Update the paths, as they are updated on kubernetes.io.
$calico1 = "https://docs.projectcalico.org/v3.3/getting-started/kubernetes/installation/hosted/rbac-kdd.yaml"
$calico2 = "https://docs.projectcalico.org/v3.3/getting-started/kubernetes/installation/hosted/kubernetes-datastore/calico-networking/1.7/calico.yaml"

# Canal yaml:
$canal1 = "https://docs.projectcalico.org/v3.3/getting-started/kubernetes/installation/hosted/canal/rbac.yaml"
# As with Flannel, must point to enp0s8 interface: $canal2 = "https://docs.projectcalico.org/v3.3/getting-started/kubernetes/installation/hosted/canal/canal.yaml"
# use local canal.yaml file, which corrects for Vagrant/VBox (points to 2nd NIC)
$canal2 = "/vagrant/canal2.yaml"

# Flannel yaml:
# Needs tweak for Vagrant: $flannel = "https://raw.githubusercontent.com/coreos/flannel/bc79dd1505b0c8681ece4de4c0d86c5cd2643275/Documentation/kube-flannel.yml", mod'd here as kube-flannel.yaml
# use local kube-flannel.yaml file, which corrects for Vagrant/VBox (points to 2nd NIC)
$flannel = "/vagrant/kube-flannel.yaml"

# Romana yaml:
# ORG $romana = "https://raw.githubusercontent.com/romana/romana/master/containerize/specs/romana-kubeadm.yml"
$romana = "/vagrant/romana-kubeadm.yaml"

# Weave yaml:
$weave = "https://cloud.weave.works/k8s/net?k8s-version=$(kubectl version | base64 | tr -d '\n')"

# Note:
# In many instances, Ruby variables can be used directly, as we can see in the servers array
#
# We use Ruby string interpolation, #{varname}, to access the master node's IP 
# address value from within, say, a HereDoc being formed (see below).
#
# The node IP addresses are calculated from the master node address, via Ruby's ".succ"
# (successor) function applied to strings
#
# E.g., if master is 192.168.205.15, then node1 will be .16, node2 .17, etc
# Vorsicht!: .succ of ".19" is ".10", not ".20"

# servers is a list of the VMs to be instantiated and configured by this script
servers = [
  {
    :name => "master",
    :type => "master",
    :box => $box,
    :box_version => $boxVer,
    :eth1 => $masterIp,
    :mem => "2048",
    :cpu => "2"
  },
  {
    :name => "node1",
    :type => "node",
    :box => $box,
    :box_version => $boxVer,
    :eth1 => $masterIp.succ,
    :mem => "2048",
    :cpu => "2"
  },
  {
    :name => "node2",
    :type => "node",
    :box => $box,
    :box_version => $boxVer,
    :eth1 => $masterIp.succ.succ,
    :mem => "2048",
    :cpu => "2"
  }
]

# Configure Ubuntu boxes
# Each VM will be configured using this script, whether kubernetes master, or merely a node
$configureUbuntu = <<SCRIPT
  # ---- BEGIN CRI (container runtime interface) install.
  # See kubernetes.io/docs/setup/docs/cri. Versions are prescribed
  # by the kubernetes.io docs. The following commands are directly from
  # "CRI Installation" for Docker.
  #
  ## If we install Docker from Ubuntu's repositories, we may not get the latest version
  ## Docker CE 17.x. (or so I've observed)  Rather, let's install directly from Docker's repo.
  ##
  ## This installs from Ubuntu:
  ## apt-get update
  ## apt-get install -y docker.io

  #  Instead, let's install Docker CE 18.06 from Docker's repositories for Ubuntu, like so:
  ## Install prerequisite packages
  apt-get update && apt-get install -y  \
    apt-transport-https                 \
    ca-certificates                     \
    curl                                \
    software-properties-common

  ## Download GPG key for the docker repo
  curl -fsSL https://download.docker.com/linux/ubuntu/gpg | apt-key add -

  ## Add apt repo for docker
  add-apt-repository \
   "deb [arch=amd64] https://download.docker.com/linux/ubuntu \
   $(lsb_release -cs) \
   stable"

  ## Install docker ce
  apt-get update && apt-get install -y docker-ce=18.06.2~ce~3-0~ubuntu
  apt-mark hold docker-ce

  # generate configuration file for docker daemon, dockerd
  cat > /etc/docker/daemon.json <<EOF
{
  "exec-opts": ["native.cgroupdriver=systemd"],
  "log-driver": "json-file",
  "log-opts": {
    "max-size": "100m"
  },
  "storage-driver": "overlay2"
}
EOF

  mkdir -p /etc/systemd/system/docker.service.d

  # Restart docker
  systemctl daemon-reload
  systemctl restart docker
  ## ---- END of CRI install/config

  # Avoid sudo for vagrant user by adding to docker group
  usermod -aG docker vagrant

  ## ---- BEGIN kubernetes.io/docs/setup/independent/install-kubeadm:
  ## Now, we apply a block of commands from kubernetes.io for kubeadm
  apt-get install -y apt-transport-https curl
  curl -s https://packages.cloud.google.com/apt/doc/apt-key.gpg | apt-key add -

  # add kubernetes.io repo to apt
  cat <<EOF >/etc/apt/sources.list.d/kubernetes.list
  deb http://apt.kubernetes.io/ kubernetes-xenial main
EOF

  echo Install kubernetes packages: kubelet, kubeadm, kubectl
  ## Install kubernetes packages, and block them from being from upgraded by apt
  apt-get update
  apt-get install -y kubelet kubeadm kubectl
  apt-mark hold kubelet kubeadm kubectl
  # ---- END kubeadm install

  echo Apply all updates
  # install all current updates
  apt-get upgrade -y

  echo Permanently enable ip_forward, bridge-nf-call-iptables
  # Apply these settings, permanently, to avoid errors in Kubeadm preflight checks
  cat >/etc/sysctl.d/99-kubernetes.conf <<EOF
net.bridge.bridge-nf-call-ip6tables = 1
net.bridge.bridge-nf-call-iptables  = 1
net.ipv4.ip_forward                 = 1
EOF

  echo Disabling swap, for now
  # kubelet requires swap off -- see "Installing kubeadm", "Before you begin"
  swapoff -a

  echo Disabling swap, forever
  # keep swap off after reboot (insert '#' at start of any line containing 'swap'
  # note: '\(' and '\)' define a remembered pattern, '\1' is the name of that pattern
  # as such, any line with ' swap ' in it is prepended with a '#'
  sudo sed -i '/ swap / s/^\(.*\)$/#\1/g' /etc/fstab

  # note: ip and grep give a line containing "n.n.n.n/24", as second field
  # awk selects the desired field, where cut trims away the trailing '/24' 
  IP_ADDR=`ip addr show dev #{$hostAdapter} | grep -i "inet " | awk '{print $2}' | cut -f1 -d/`

  # Our k8s VMs each have two network adapters, first one is NAT, 2nd is host-only
  # Kubeadm assumes the first adapter is the one whose IP addr is to be used by K8s
  # The NAT address is the same for all VMs, and will not work.
  # As such, we must specify the host-only IP address for kubernetes to use
  #
  # set node-ip in the relevant kubelet file under /etc, then restart the kubelet
  sudo sed -i "/^[^#]*KUBELET_EXTRA_ARGS=/c\KUBELET_EXTRA_ARGS=--node-ip=$IP_ADDR" #{$cfgKubelet}
  sudo systemctl restart kubelet

  # NFS client to be available everywhere:
  apt-get install -y nfs-common

  # Copy key-pair from host's project directory to vagrant user's .ssh directory on node
  cp /vagrant/id_rsa /home/vagrant/.ssh
  if [[ $? -ne 0 ]]; then
    echo "ERROR: Error copying /vagrant/id_rsa key"
  fi
  cp /vagrant/id_rsa.pub /home/vagrant/.ssh
  if [[ $? -ne 0 ]]; then
    echo "ERROR: Error copying /vagrant/id_rsa key"
  fi
  chown $(id -u vagrant):$(id -g vagrant) /home/vagrant/.ssh/id_rsa
  chown $(id -u vagrant):$(id -g vagrant) /home/vagrant/.ssh/id_rsa.pub
  echo "vagrant private/public keys installed"
SCRIPT
# End Ubuntu Configuration Script

# Generate script to build master nodes
$configureMaster = <<SCRIPT
  # install k8s master
  echo "This is a Kubernetes master node"

  # Fetch IP address of this box from the host network adapter info, as above
  IP_ADDR=`ip addr show dev #{$hostAdapter} | grep -i "inet " | awk '{print $2}' | cut -f1 -d/`
  echo "host adapter, #{$hostAdapter}, has address $IP_ADDR"

  HOST_NAME=$(hostname -s)

  echo "Running kubeadm init -- creating cluster"
  cmd="kubeadm init --apiserver-advertise-address=$IP_ADDR #{$netCidr}"
  echo "cmd: $cmd"
  $cmd

  sudo --user=vagrant mkdir -p /home/vagrant/.kube
  cp -i /etc/kubernetes/admin.conf /home/vagrant/.kube/config
  chown $(id -u vagrant):$(id -g vagrant) /home/vagrant/.kube/config

  echo "Defined KUBECONFIG, which kubectl uses to obtain vital cluster information"
  export KUBECONFIG=/etc/kubernetes/admin.conf

  echo "Install network for our cluster"
  if [[ #{$net} = "flannel" ]]; then
    echo "net: #{$net}. installing flannel"
    kubectl apply -f #{$flannel}
  elif [[ #{$net} = "weave" ]]; then
    echo "net: #{$net}. installing weave"

    # Weave BUG workaround: by default, no route from nodes to master.
    # Symptoms:
    #   Nodes NotReady; weave pods in CrashLoopBackoff
    #   Node logs show: "Failed to get peers" (kubectl logs -n kube-system -f weave-net-spx6g weave)
    # One soln promoted online is to add a route to the master node, as we do here:

    # For Ubuntu, append a line to the end of /etc/network/interfaces, and restart the interface
    echo "      up ip route add 10.96.0.1/32 via #{$masterIp}" >>/etc/network/interfaces
    ifdown enp0s8 && ifup enp0s8

    ## For CentOS, add a file containing the desired route
    #echo "10.96.0.1 via #{$masterIp} dev eth1" >./route-eth1
    #sudo mv ./route-eth1 /etc/sysconfig/network-scripts/
    #sudo systemctl restart network

    kubectl apply -f #{$weave}
  elif [[ #{$net} = "calico" ]]; then
    echo "net: #{$net}. installing calico"
    kubectl apply -f #{$calico1}
    kubectl apply -f #{$calico2}
  elif [[ #{$net} = "canal" ]]; then
    echo "net: #{$net}. installing canal"
    kubectl apply -f #{$canal1}
    kubectl apply -f #{$canal2}
  elif [[ #{$net} = "romana" ]]; then
    echo "net: #{$net}. installing romana"
    kubectl apply -f #{$romana}
  fi

  # Generate join script to add nodes to cluster
  echo "Generate a join script for the other nodes to run"
  kubeadm token create --print-join-command >> /etc/kubeadm_join_cmd.sh
  chmod +x /etc/kubeadm_join_cmd.sh

  # Add the public key to vagrant user's authorized_keys file
  echo "Append the vagrant user's public key to the authorized_keys file"
  cat /home/vagrant/.ssh/id_rsa.pub >>/home/vagrant/.ssh/authorized_keys
SCRIPT
# End of script to build master nodes

# Generate script to configure worker nodes
$configureNode = <<SCRIPT
  echo "I am a node, not a master"

  if [[ #{$net} = "flannel" ]]; then
    echo "net: #{$net}. "
  elif [[ #{$net} = "romana" ]]; then
    echo "net: #{$net}. "
  elif [[ #{$net} = "weave" ]]; then
    echo "net: #{$net}."

    # Weave BUG workaround: by default, no route from nodes to master.
    # Symptoms:
    #   Nodes NotReady; weave pods in CrashLoopBackoff
    #   Node logs show: "Failed to get peers" (kubectl logs -n kube-system -f weave-net-spx6g weave)
    # One soln promoted online is to add a route to the master node, as we do here:

    ## For Ubuntu, append a line to the end of /etc/network/interfaces, and restart the interface
    echo "      up ip route add 10.96.0.1/32 via #{$masterIp}" >>/etc/network/interfaces
    ifdown enp0s8 && ifup enp0s8

    ## For CentOS, add a file containing the desired route
    #echo "10.96.0.1 via #{$masterIp} dev eth1" >./route-eth1
    #sudo mv ./route-eth1 /etc/sysconfig/network-scripts/
    #sudo systemctl restart network
  fi

  echo "Copy join script from master node to local directory"
  scp -i /home/vagrant/.ssh/id_rsa -o StrictHostKeyChecking=no vagrant@#{$masterIp}:/etc/kubeadm_join_cmd.sh .
  echo "Run join script to join cluster"
  sh ./kubeadm_join_cmd.sh
SCRIPT

# Configure each VBox VM 
Vagrant.configure("2") do |config|
  # suggested on stack overflow, 46150672, avoiding issue with WSL (not needed on Linux)
  config.ssh.insert_key = false
  servers.each do |opts|
    config.vm.define opts[:name] do |config|
      config.vm.box         = opts[:box]
      config.vm.box_version = opts[:box_version]
      config.vm.hostname    = opts[:name]
      config.vm.network :private_network, ip: opts[:eth1]
      config.vm.provider "virtualbox" do |vb|
        vb.name = opts[:name]
        vb.customize ["modifyvm", :id, "--groups", "/Kubernetes #{$grpSuffix}"]
        vb.customize ["modifyvm", :id, "--memory",  opts[:mem]]
        vb.customize ["modifyvm", :id, "--cpus",    opts[:cpu]]
        # The following avoids errors when WSL tries to open logfile when creating Ubuntu/Xenial
        # (not needed if hosted on Linux -- only Windows)
        vb.customize ['modifyvm', :id, '--uartmode1', 'disconnected']
      end

      # base config
      config.vm.provision "shell", inline: $configureUbuntu

      # morph into master/worker nodes
      if opts[:type] == "master"
        config.vm.provision "shell", inline: $configureMaster
      else
        config.vm.provision "shell", inline: $configureNode
      end

      # stage some files from the host to /tmp on the nodes
      config.vm.provision "file", source: "~/.profile", destination: "/tmp/.profile"
      config.vm.provision "file", source: "~/.bashrc",  destination: "/tmp/.bashrc"
      # config.vm.provision "file", source: "~/.vimrc",   destination: "/tmp/.vimrc"

      # Here, we apply a bit of "Ruby dust", fetching the LOGNAME from the current environment,
      # and passing that to our post-k8s script as an environment variable, NEWUSER, appearing
      # in the shell we're spawning on each VM, the credentials of which will be installed,
      # downloaded from the mounted project directory in /vagrant.
      # Those credentials will allow our host user to ssh to the VMs.
      config.vm.provision "shell", path: "post-k8s.sh", env: {"NEWUSER" => ENV['LOGNAME']}
    end
  end
end
